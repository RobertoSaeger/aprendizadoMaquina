# -*- coding: utf-8 -*-
"""Ativ2_PedroGemalLanzieri_v2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WZxrIy1jJKL88z9Xoktsrr0uaK5vkET5

###Setup do ambiente
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd 
from pandas import Series, DataFrame

import seaborn as sns
import matplotlib.pyplot as plt

# %matplotlib inline

import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)

url = 'https://raw.githubusercontent.com/pedrogemal/pgc-ml-2021-1/main/dataset/Iris.csv'
iris = pd.read_csv(url)

iris.head()

iris.info()

"""### Tratamento dos dados"""

iris.drop("Id", axis=1, inplace = True)

"""## Atividade

Usar o dataset Iris para executar os algoritmos K-NN, Árvores de Decisão, NaiveBayes, SVM e MLP para construir classificadores. 

Separe o conjunto de dados iris em 2/3 para treinamento e 1/3 para teste. 

Use a mesma partição para todos os algoritmos. Variar o valor de K para 1, 3, 5, 7 e 9. 
Variar os parâmetros de SVM e MLP usando a função Randomized Search do Scikit ou o GridSearchCV.

Usar as implementações do Scikit. 

Analise os valores das métricas de acurácia (accuracy), precisão (precision), recall e F1.
"""

# Importação das bibliotecas para execução dos algoritmos
from sklearn.linear_model import LogisticRegression 
from sklearn.model_selection import train_test_split 
from sklearn.neighbors import KNeighborsClassifier 
from sklearn import svm 
from sklearn.tree import DecisionTreeClassifier 

# Importação das bibliotecas para métricas
from sklearn import metrics 
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix

# Para ganhar tempo, vou importar o dataset diretamente do sklearn porque o CSV que utilizei precisaria de um tratamento adicional de dados
from sklearn.datasets import load_iris

iris.shape

"""### Análise de correlação de features (para seleção).

A largura e o comprimento da sépala não estão correlacionados; já a largura e o comprimento da pétala estão altamente correlacionados. Usarei todas as features para treinar o algoritmo e verificar a precisão. Em seguida, usarei uma feature de pétala e uma feature de sépala para verificar a precisão do algoritmo, pois dispomos de apenas 2 features que não estão correlacionadas.
"""

plt.figure(figsize=(8,4))
sns.heatmap(iris.corr(), annot=True, cmap='cubehelix_r') 
plt.show()

"""###Separação dos dados para treinamento e teste"""

train, test = train_test_split(iris, test_size=0.33)

print(train.shape)
print(test.shape)

train_X = train[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']] 
train_y = train.Species

test_X = test[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']]
test_y = test.Species

train_X.head()

test_X.head()

train_y.head()

"""###Support Vector Machine (SVM)"""

model = svm.SVC() 

model.fit(train_X, train_y)

prediction = model.predict(test_X)
print('A acuracia para SVM foi: ', metrics.accuracy_score(prediction, test_y))

"""###SVM com GridSearch"""

from sklearn.model_selection import GridSearchCV
  
# defining parameter range
param_grid = {'C': [0.1, 1, 10, 100, 1000], 
              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],
              'kernel': ['rbf']} 
  
grid = GridSearchCV(svm.SVC(), param_grid, refit = True)
  
# ajustando o modelo para grid search
grid.fit(train_X, train_y)

# imprimir o melhor parâmetro após o ajuste
# print(grid.best_params_)
  
# modelo após o ajuste de hiperparâmetro
# print(grid.best_estimator_)

grid_predictions = grid.predict(test_X)
  
print('\n SVM com GridSearch: \n')
print(classification_report(test_y, grid_predictions))

"""### Decision Tree"""

model = DecisionTreeClassifier()
model.fit(train_X, train_y)
prediction = model.predict(test_X)
print('A acuracia para Decision Tree foi: ', metrics.accuracy_score(prediction, test_y))

"""### K-Nearest Neighbors"""

model = KNeighborsClassifier(n_neighbors=3) 
model.fit(train_X, train_y)
prediction = model.predict(test_X)
print('A acuracia para KNN foi: ', metrics.accuracy_score(prediction, test_y))

"""### Variando valor de k para K-Nearest nerighbours

"""

a_index = list(range(1,11))
a = pd.Series()
for i in list(range(1,11)):
    model = KNeighborsClassifier(n_neighbors=i)
    model.fit(train_X, train_y)
    prediction = model.predict(test_X)
    a = a.append(pd.Series(metrics.accuracy_score(prediction, test_y)))
plt.plot(a_index, a)
x = [1,3,5,7,9]
plt.xticks(x)

"""###NaiveBayes"""

# Gaussian Naive Bayes
from sklearn.naive_bayes import GaussianNB
classifier = GaussianNB()
classifier.fit(train_X, train_y)

y_pred = classifier.predict(test_X)

print(classification_report(test_y, y_pred))
print(confusion_matrix(test_y, y_pred))

from sklearn.metrics import accuracy_score
print('A acuracia para Gaussian Naive Bayes foi',accuracy_score(y_pred,test_y))

# Multinomial Naive Bayes
from sklearn.naive_bayes import MultinomialNB
classifier = MultinomialNB()
classifier.fit(train_X, train_y)

y_pred = classifier.predict(test_X)

print(classification_report(test_y, y_pred))
print(confusion_matrix(test_y, y_pred))

from sklearn.metrics import accuracy_score
print('A acuracia para Multinomial Naive Bayes foi',accuracy_score(y_pred,test_y))

# Bernoulli Naive Bayes
from sklearn.naive_bayes import BernoulliNB
classifier = BernoulliNB()
classifier.fit(train_X, train_y)

y_pred = classifier.predict(test_X)

print(classification_report(test_y, y_pred))
print(confusion_matrix(test_y, y_pred))

from sklearn.metrics import accuracy_score
print('A acuracia para Bernoulli Naive Bayes foi',accuracy_score(y_pred,test_y))

# Complement Naive Bayes
from sklearn.naive_bayes import ComplementNB
classifier = ComplementNB()
classifier.fit(train_X, train_y)

y_pred = classifier.predict(test_X)

print(classification_report(test_y, y_pred))
print(confusion_matrix(test_y, y_pred))

from sklearn.metrics import accuracy_score
print('A acuracia para Complement Naive Bayes foi',accuracy_score(y_pred,test_y))

"""###Multilayer Perceptron (MLP)"""

from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier

scaler = StandardScaler()

scaler.fit(train_X)

train_data = scaler.transform(train_X)
test_data = scaler.transform(test_X)

mlp = MLPClassifier(hidden_layer_sizes=(10, 5), max_iter=1000)

mlp.fit(train_X, train_y)

predictions_train = mlp.predict(train_data)

print('Acuracia (treinamento): ')
print(accuracy_score(predictions_train, train_y))

predictions_test = mlp.predict(test_data)

print('\nAcuracia (teste): ')
print(accuracy_score(predictions_test, test_y))

print('\nResultado final:\n')
print(classification_report(predictions_test, test_y))

"""###MLP com GridSearch"""

#from sklearn.neural_network import MLPClassifier
#from sklearn.model_selection import GridSearchCV

grid = {'solver': ['lbfgs', 'sgd', 'adam'], 'activation': ['identity', 'logistic', 'tanh', 'relu']}
clf_cv = GridSearchCV(MLPClassifier(random_state=1, max_iter=5000, hidden_layer_sizes=(3,3), alpha=1e-5), grid, n_jobs=-1, cv=10)

clf_cv.fit(train_X, train_y)

print("GridSearch():\n")
combination = 1
for x in grid.values():
    combination *= len(x)
print('Existem {} combinações para o GridSearch'.format(combination))
print("Melhor configuracao: ",clf_cv.best_params_)
best_config_gs = clf_cv.best_params_
print("Acuracia CV:",clf_cv.best_score_)
ppn_cv = clf_cv.best_estimator_
print('Acuracia teste: %.3f' % clf_cv.score(test_X, test_y))

mlp = MLPClassifier(random_state=1, max_iter=5000, hidden_layer_sizes=(3,3), alpha=1e-5, **best_config_gs)

mlp.fit(train_X,train_y)
predict_train = mlp.predict(train_X)
predict_test = mlp.predict(test_X)

#Matriz de confusão e relatório de classificação para dados de treino
from sklearn.metrics import classification_report,confusion_matrix
print(confusion_matrix(train_y,predict_train))
print(classification_report(train_y,predict_train))

#Matriz de confusão e relatório de classificação para dados de teste
print(confusion_matrix(test_y,predict_test))
print(classification_report(test_y,predict_test))